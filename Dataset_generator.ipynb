{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2bbeebb-cb5f-4d3b-a196-9a88b1b1a6d2",
   "metadata": {},
   "source": [
    "<h1>Dataset Generator</h1>\n",
    "<p>A powerful tool leveraging Hugging Face LLM and Gradio to generate high-quality datasets for AI applications.</p>\n",
    "\n",
    "<p>Features:</p>\n",
    "<ul>\n",
    "<li>Uses state-of-the-art LLM models from Hugging Face.</li>\n",
    "<li>Interactive UI powered by Gradio.</li>\n",
    "<li>stomizable dataset generation for various testing tasks.</li>\n",
    "<li>Supports text completion, classification, and summarization.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d94658-2409-411c-bef8-47601f611fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers gradio torch accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb65384-e6ba-419b-a8c2-c02924695c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig, TextStreamer\n",
    "import gradio as gr\n",
    "from huggingface_hub import login\n",
    "from google.colab import userdata\n",
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca968c8-8127-45b2-9a42-1fee77eed554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in to HuggingFace\n",
    "\n",
    "hf_token = userdata.get('HF_TOKEN2')\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e3c529-5b52-4a97-84c2-f74585f31eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Model\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_name,\n",
    "  device_map=\"auto\",\n",
    "  quantization_config=quant_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ba9551-3869-46fb-952d-8ca66d4b3676",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b1135a-0b3a-44c0-a80f-1764fe3d8c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation function\n",
    "\n",
    "def generate_dataset(num_fields, *args):\n",
    "    max_fields = 8\n",
    "    params = list(args)\n",
    "    \n",
    "    # Split parameters correctly\n",
    "    field_names = params[:max_fields]\n",
    "    field_types = params[max_fields:2*max_fields]\n",
    "    other_params = params[2*max_fields:]\n",
    "    \n",
    "    # Extract generation parameters\n",
    "    num_entries = other_params[0]\n",
    "    user_prompt = other_params[1]\n",
    "    output_format = other_params[2]  # Should be string value\n",
    "    \n",
    "    # Validate fields\n",
    "    fields = []\n",
    "    for i in range(num_fields):\n",
    "        name = field_names[i].strip()\n",
    "        ftype = field_types[i]\n",
    "        if not name:\n",
    "            return None, None, f\"Field {i+1} name cannot be empty\"\n",
    "        fields.append((name, ftype))\n",
    "\n",
    "    # Build system prompt\n",
    "    system_prompt = f\"\"\"Generate {num_entries} entries as JSON array. Each object must have:\"\"\"\n",
    "    for name, ftype in fields:\n",
    "        system_prompt += f\"\\n- {name} ({ftype})\"\n",
    "    system_prompt += \"\\nOutput ONLY the JSON array with no additional text.\"\n",
    "\n",
    "    try:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "\n",
    "        # Apply Llama 3 chat template\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "\n",
    "        # Generate output\n",
    "        generation_params = {\n",
    "            \"input_ids\": inputs,\n",
    "            \"max_new_tokens\": 2000,\n",
    "            \"temperature\": 0.7,\n",
    "            \"do_sample\": True,\n",
    "            \"top_p\": 0.9,\n",
    "            \"pad_token_id\": tokenizer.eos_token_id\n",
    "        }\n",
    "\n",
    "        print(\"⚡ Starting generation...\")\n",
    "        outputs = model.generate(**generation_params)\n",
    "        \n",
    "        # Decode and clean output\n",
    "        full_output = tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True)\n",
    "        cleaned = full_output.strip()\n",
    "\n",
    "        # Parse and validate JSON\n",
    "        data = json.loads(cleaned)\n",
    "        if not isinstance(data, list):\n",
    "            raise ValueError(\"Output must be a JSON array\")\n",
    "\n",
    "        # Validate fields\n",
    "        expected_fields = [name for name, _ in fields]\n",
    "        for entry in data:\n",
    "            if set(entry.keys()) != set(expected_fields):\n",
    "                raise ValueError(\"Generated fields don't match specification\")\n",
    "\n",
    "        # Save file\n",
    "        filename = f\"dataset_{datetime.now().strftime('%Y%m%d%H%M%S')}.{output_format}\"\n",
    "        keys = [name for name, _ in fields]\n",
    "        \n",
    "        if output_format == \"json\":\n",
    "            with open(filename, \"w\") as f:\n",
    "                json.dump(data, f, indent=2)\n",
    "        elif output_format == \"csv\":\n",
    "            with open(filename, \"w\", newline=\"\") as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=keys)\n",
    "                writer.writeheader()\n",
    "                writer.writerows(data)\n",
    "        elif output_format == \"jsonl\":\n",
    "            with open(filename, \"w\") as f:\n",
    "                for item in data:\n",
    "                    f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "        return data, filename, None\n",
    "\n",
    "    except json.decoder.JSONDecodeError as e:\n",
    "        return None, None, f\"JSON Parsing Error: {str(e)}\\nRaw Output: {cleaned}\"\n",
    "    except Exception as e:\n",
    "        return None, None, f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6991563f-bbcf-4b73-a9cb-4d97a54bb720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio Interface\n",
    "max_fields = 8\n",
    "default_names = ['id', 'name', 'email', 'age', 'city', 'is_active', 'score', 'birthdate']\n",
    "default_types = ['number', 'string', 'string', 'number', 'string', 'boolean', 'number', 'string']\n",
    "\n",
    "with gr.Blocks() as app:\n",
    "    gr.Markdown(\"# 🧩 Custom Dataset Generator\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"## 🔧 Field Configuration\")\n",
    "            num_fields = gr.Slider(1, max_fields, value=4, step=1, label=\"Number of Fields\")\n",
    "            \n",
    "            field_rows = []\n",
    "            field_names = []\n",
    "            field_types = []\n",
    "            for i in range(max_fields):\n",
    "                with gr.Row(visible=(i < 4)) as row:\n",
    "                    name = gr.Textbox(\n",
    "                        label=f\"Field {i+1} Name\", \n",
    "                        value=default_names[i] if i < len(default_names) else \"\",\n",
    "                        interactive=True\n",
    "                    )\n",
    "                    ftype = gr.Dropdown(\n",
    "                        [\"string\", \"number\", \"boolean\"], \n",
    "                        label=f\"Field {i+1} Type\",\n",
    "                        value=default_types[i] if i < len(default_types) else \"string\",\n",
    "                        interactive=True\n",
    "                    )\n",
    "                    field_names.append(name)\n",
    "                    field_types.append(ftype)\n",
    "                    field_rows.append(row)\n",
    "\n",
    "            num_fields.change(\n",
    "                lambda n: [gr.Row.update(visible=i < n) for i in range(max_fields)],\n",
    "                inputs=num_fields,\n",
    "                outputs=field_rows\n",
    "            )\n",
    "\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"## ⚙️ Generation Parameters\")\n",
    "            num_entries = gr.Dropdown(\n",
    "                choices=[15, 50, 100, 150, 200, 250],  \n",
    "                value=15,  \n",
    "                label=\"Number of Entries\"\n",
    "            )\n",
    "            user_prompt = gr.Textbox(\n",
    "                label=\"Content Instructions\", \n",
    "                lines=3,\n",
    "                value=\"Generate realistic data entries with:\"\n",
    "            )\n",
    "            output_format = gr.Radio(\n",
    "                [\"json\", \"csv\", \"jsonl\"], \n",
    "                label=\"Output Format\", \n",
    "                value=\"json\"\n",
    "            )\n",
    "            generate_btn = gr.Button(\"🚀 Generate Dataset\", variant=\"primary\")\n",
    "\n",
    "    with gr.Row():\n",
    "        data_preview = gr.JSON(label=\"📊 Data Preview\")\n",
    "        file_output = gr.File(label=\"📥 Download File\")\n",
    "        error_output = gr.Textbox(label=\"❌ Error Messages\", visible=False)\n",
    "\n",
    "    generate_btn.click(\n",
    "        generate_dataset,\n",
    "        inputs=[num_fields] + field_names + field_types + [num_entries, user_prompt, output_format],\n",
    "        outputs=[data_preview, file_output, error_output]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab78235-da77-43dc-b31d-e6724face1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "app.launch(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35351997-c755-4002-9420-6b19a34ca992",
   "metadata": {},
   "source": [
    "<h2>Use Cases</h2>\n",
    "\n",
    "<p>This dataset generator is ideal for:</p>\n",
    "\n",
    "<ul>\n",
    "<li>Training chatbots and virtual assistants.</li>\n",
    "<li>Developing sentiment analysis models.</li>\n",
    "<li>Creating summarization datasets for AI research.</li>\n",
    "<li>Fine-tuning custom NLP applications.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917b48a2-d1b5-4a05-a4dc-abfd51c95f6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
